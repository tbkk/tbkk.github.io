---
layout:     post
title:      大数据组件之HBASE-架构
subtitle:   HBase
date:       2019-01-19
author:     TBKK
header-img: img/post-bg-hacker.jpg
catalog: true
tags:
    - 大数据
---


# HBase整体架构

![image](http://www.qinxinfeng.com/img/hbase/6.jpg)
这张架构图，在各大网站均可看到，下面我们分析一下核心组件，虚线一下其实属于HDFS范围了，属于底层支撑了。这种关系举个例子，相当于Mysql之于文件系统,我们只需要看虚线以上的就行了。


### HBase核心组件

#### Client：
* 使用HBase RPC机制与HMaster和HRegionServer进行通信
* Client与HMaster进行通信进行管理类操作 
* Client与HRegionServer进行数据读写类操作 

#### Zookeeper：
* Zookeeper Quorum存储-ROOT-表地址、HMaster地址 
* HRegionServer把自己以Ephedral方式注册到Zookeeper中，HMaster随时感知各个HRegionServer的健康状况 
* Zookeeper避免HMaster单点问题 

#### HMaster：主要负责Table和Region的管理工作
* 管理用户对表的增删改查操作 
* 管理HRegionServer的负载均衡，调整Region分布 
* Region Split后，负责新Region的分布 
* 在HRegionServer停机后，负责失效HRegionServer上Region迁移 

#### HRegionServer：
* 负责HRegion管理工作

#### HStore：
* HBase存储的核心。由MemStore和StoreFile组成。 

#### HLog：
* 每次用户操作写入Memstore的同时，也会写一份数据到HLog文件

#### HFile：
* 数据文件都存储在Hadoop HDFS文件系统上

### HBase 写数据

![image](http://www.qinxinfeng.com/img/hbase/7.jpg)

* 先写入WAL（write ahead log），WAL存放在HDFS上
* 每次Put、Delete操作的数据均追加到WAL末端。
* 持久化到WAL之后，再写到MemStore中 
* 两者写完返回ACK到客户端


### HBase Region FLUSH
![image](http://www.qinxinfeng.com/img/hbase/8.jpg)
* 单个RS中所有的MemStore使用达到上限（默认超过堆的40%） 
* 单个Region中的MemStore超过大小
* RegionServer的Hlog数量达到上限 。
* 手动触发 。

### HBase Compaction
![image](http://www.qinxinfeng.com/img/hbase/9.jpg)
* Minor Compaction: 
1. 指选取一些小的相邻的HFile将他们合并成一个更大的Hfile。
* Major Compaction：
1. 将一个CF 下所有的 Hfiles 合并成更大的；
2. 删除那些被标记为删除的数据、超过TTL（time-to-live）时限的数据，以及超过了版本数量限制的数据。

### HBase Split
![image](http://www.qinxinfeng.com/img/hbase/10.jpg)
* 可以配置不同的Split规则
* 主要看Region大小

### HBase 读数据
![image](http://www.qinxinfeng.com/img/hbase/11.jpg)

### HBase 如何找到数据

![image](http://www.qinxinfeng.com/img/hbase/12.jpg)

## 总结
本文主要是讲的HBase架构，适用于需要HBase调优的人群。或者常年与HBase打交道的人。
 


 

